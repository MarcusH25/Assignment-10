# Assignment 10: Data Bias (Coding) 
Analysis and Documentation:

Decide how you would like to test the Perspective model for bias. Document your methods, all queries that you make to the API, and all scores received.

Test Method: 
I tested for bias in the Perspective API model to findout weather or not changing the writing styles and words would effect the toxicity scores. I used the perspective API and called the get toxcity score function to get the greatest and lowest toxicity ratings, then changed the writing styles and words in the phrases to see how it would impacted the scores. Next I examined the findings to see if there were any trends or biases in the toxicity levels.
 
Write a few paragraphs, either in the README or in the notebook, reflecting on what you have learned, what you found, what (if anything) surprised you about your findings, and/or what theories you have about why any biases might exist, if you find they exist. You can also include any questions this assignment raised for you about bias or machine learning. Questions you may wish to answer include:

I examined the impact of different writing styles and words more specifically, words that had to do with culture and race. I then analyzed how the level of the scores changed and what caused it. I found that certain writing styles do change the level of the toxicity score. I noticed that changing phrases from lowercase to uppercase did not affect the score whatsoever. This surprised me, as before I tested my theory, I believed that capitalization would have one of the biggest influences on the toxicity score. The only instance where I found increasing the tone of the text increased the score was when using an exclamation point, which would slightly increase the toxicity score. I found the highest rise in toxcity score, to be phrases that used culturally specific words. I discovered the more culturally focused the phrase the higher your score will be even if the sentence isn't used in an instance of being harmful. Lastly, I tested my theory of whether writing style affects the scores. I tested this in two ways first I used such writing styles as, formal, Informal, Conversational, and Persuasive to see if that would increase the toxicity score. I found the writing style of conversational to have the greatest impact and the Informal style to have the least. As these two styles did change the score the impact was minor. Next, I tested how spaces, profanity, and commas would affect the score. I found that more spaces meant a lower overall score. Commas slightly increase your score but only in certain instances. But my biggest discovery was found using words with profanity as adding these words raises the score no matter what. I also found the model to be biased because of this, as the model does not understand what instance the phrase is being used in such as a tweet, comment, or post, I believe there to be quite a bias when outputting the toxicity scores as it does not know the true origin of the phrase. It's also biased in the way it gives a higher value to words with profanity. For instance, a phrase said with profanity but in good context will still be given a high toxicity score. 

What biases do you think might exist in the model based on intuitions or public documentation about how the model was created?

The model has a few biases. For starters, it values culturally specific terms, giving them higher toxicity value even in harmless contexts. Second, it does not comprehend the context of phrases, resulting in higher scores. The model is highly sensitive to different writing styles, and the type of style you are writing will result in different scores based on the formality of the style. Lastly, I believe how the data was collected is quite a problem. As the CVS file does not have a collum stating how the phrase was collected it is hard for the model to not have biases as it's impossible to understand what instance it's being used in.


What were your results?

My findings showed that certain writing styles affect the toxicity score, with conversational style having the most effect and informal style having the least. Exclamation points somewhat raised the scores, while capitalization had no effect. Even in non-harmful circumstances, culturally distinctive terms caused the greatest increase in toxicity ratings. Additional spaces led to poorer overall scores, but commas raised scores slightly in some cases. Regardless of the context, profanity greatly increased the scores. Biases was evident in the model, especially in how it handled phrases with culturally specific words. 

